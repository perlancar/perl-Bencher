#!perl

use 5.010001;
use strict;
use warnings;

use Perinci::CmdLine::Any;

# AUTHORITY
# DATE
# DIST
# VERSION

binmode(STDOUT, ":encoding(utf8)");
Perinci::CmdLine::Any->new(
    url => '/Bencher/Backend/bencher',
    extra_urls_for_version => ['/Bencher/Backend/', '/Bencher/'],
    log => 1,
    pass_cmdline_object => 1,
)->run;

# ABSTRACT: A benchmark framework (CLI)
# PODNAME:

=head1 SYNOPSIS

# BEGIN_CODE
# show generated usage line

require Bencher::Backend;
require Perinci::CmdLine::Dump;

my $res = Perinci::CmdLine::Dump::dump_pericmd_script(filename=>"script/bencher", libs=>["lib"]);
die "Can't dump script/bencher: $res->[0] - $res->[1]" unless $res->[0] == 200;
my $cli = $res->[2];

require Perinci::Sub::To::CLIDocData;
$res = Perinci::Sub::To::CLIDocData::gen_cli_doc_data_from_meta(meta => $Bencher::Backend::SPEC{bencher}, common_opts => $cli->{common_opts});
die "Can't gen_cli_doc_data_from_meta(): $res->[0] - $res->[1]" if $res->[0] != 200;
my $usage = $res->[2]{'usage_line.alt.fmt.pod'};
$usage =~ s/\[\[prog\]\]/B<bencher>/;
print "$usage\n\n";

# END_CODE

=head2 Getting started and basic usage

To benchmark things, you write a I<scenario> file. Let's write a simple one that
benchmarks several trim functions. In F<scenario.pl>:

 #!/usr/bin/env perl

 use strict;
 use warnings;

 our $scenario = {
     participants => [
         {fcall_template=>'String::Trim::NonRegex::trim(<str>)'},
         {fcall_template=>'String::Trim::Regex::trim(<str>)'},
         {fcall_template=>'Text::Minify::XS::minify_ascii(<str>)'},
     ],
     datasets => [
         {name=>'empty'        , args=>{str=>''}},
         {name=>'len10ws1'     , args=>{str=>' '.('x' x   10).' '}},
         {name=>'len100ws1'    , args=>{str=>' '.('x' x  100).' '}},
     ],
 };

The scenario is declared in package variable C<$scenario>. In the above script,
we define three I<participants> (function/code that will be benchmarked) and
three I<datasets> (arguments to functions): an empty string, a 10-character
string surrounded by two spaces, and a 100-character string surrounded by two
spaces.

You'll notice that the participant is a string (a code template) instead of
coderef. Bencher can benchmark coderef, but by using a code template, we can
permute the code with different datasets into benchmark I<items> (each item is
the actual benchmark code that will be run). (For more terminologies and
concepts, see L<Bencher>, but the abovementioned terms are pretty much all the
important ones.)

Next, run the scenario (do the benchmarking):

 % bencher -f scenario.pl
 # Run on: perl v5.34.0, CPU Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz (2 cores), OS GNU/Linux Ubuntu version 20.04, OS kernel: Linux version 5.4.0-91-generic
 # Elapsed time: 0.29s
 +--------------------------------+-----------+-----------+-----------+-----------------------+-----------------------+---------+---------+
 | participant                    | dataset   | rate (/s) | time (μs) | pct_faster_vs_slowest | pct_slower_vs_fastest |  errors | samples |
 +--------------------------------+-----------+-----------+-----------+-----------------------+-----------------------+---------+---------+
 | String::Trim::Regex::trim      | len100ws1 |    165330 |   6.04851 |                 0.00% |              4531.91% | 5.4e-12 |      20 |
 | String::Trim::Regex::trim      | len10ws1  |    862720 |   1.1591  |               421.82% |               787.65% | 5.7e-12 |      24 |
 | String::Trim::NonRegex::trim   | len100ws1 |   1048000 |   0.9539  |               534.07% |               630.51% | 1.7e-11 |      20 |
 | String::Trim::NonRegex::trim   | len10ws1  |   1060000 |   0.946   |               539.58% |               624.21% | 4.2e-10 |      20 |
 | String::Trim::NonRegex::trim   | empty     |   1580000 |   0.633   |               856.02% |               384.50% | 1.9e-10 |      24 |
 | Text::Minify::XS::minify_ascii | len100ws1 |   2737000 |   0.3653  |              1555.73% |               179.75% | 5.8e-12 |      20 |
 | Text::Minify::XS::minify_ascii | len10ws1  |   6640000 |   0.1506  |              3916.37% |                15.33% | 5.7e-12 |      20 |
 | String::Trim::Regex::trim      | empty     |   7100000 |   0.14    |              4180.07% |                 8.22% | 1.6e-10 |      20 |
 | Text::Minify::XS::minify_ascii | empty     |   7700000 |   0.13    |              4531.91% |                 0.00% | 2.6e-10 |      20 |
 +--------------------------------+-----------+-----------+-----------+-----------------------+-----------------------+---------+---------+

From the result, we see that L<String::Trim::Regex> is slower than
L<String::Trim::NonRegex>. But both are obliterated by L<Text::Minify::XS>,
which is no surprise since it is an XS module.

You'll notice that the benchmark runs pretty quickly because it uses
L<Benchmark::Dumb> by default, so each code only needs to be run a few times
instead of seconds-long like with L<Benchmark>.pm. You'll also notice the output
is in table form, showing the code from slowest to fastest. If you prefer the
Benchmark.pm matrix output, use the C<-B> option:

 % bencher -f scenario.pl -B
 # Run on: perl v5.34.0, CPU Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz (2 cores), OS GNU/Linux Ubuntu version 20.04, OS kernel: Linux version 5.4.0-91-generic
 # Elapsed time: 0.20s
                          Rate  STR:t len100ws1  STR:t len10ws1  STN:t len100ws1  STN:t len10ws1  STN:t empty  TMX:m_a len100ws1  TMX:m_a len10ws1  STR:t empty  TMX:m_a empty
  STR:t len100ws1     155160/s               --            -81%             -85%            -85%         -90%               -94%              -97%         -97%           -98%
  STR:t len10ws1      846000/s             446%              --             -19%            -19%         -46%               -68%              -87%         -88%           -89%
  STN:t len100ws1    1050000/s             576%             23%               --              0%         -33%               -61%              -84%         -85%           -87%
  STN:t len10ws1     1100000/s             578%             24%               0%              --         -33%               -61%              -84%         -85%           -87%
  STN:t empty        1600000/s             922%             87%              51%             50%           --               -41%              -76%         -78%           -80%
  TMX:m_a len100ws1  2703000/s            1642%            219%             157%            156%          70%                 --              -59%         -62%           -67%
  TMX:m_a len10ws1   6703000/s            4219%            690%             538%            536%         322%               147%                --          -8%           -19%
  STR:t empty        7320000/s            4604%            761%             594%            593%         359%               169%                8%           --           -11%
  TMX:m_a empty      8277060/s            5234%            876%             687%            686%         421%               206%               23%          13%             --

 Legends:
   STN:t empty: dataset=empty participant=String::Trim::NonRegex::trim
   STN:t len100ws1: dataset=len100ws1 participant=String::Trim::NonRegex::trim
   STN:t len10ws1: dataset=len10ws1 participant=String::Trim::NonRegex::trim
   STR:t empty: dataset=empty participant=String::Trim::Regex::trim
   STR:t len100ws1: dataset=len100ws1 participant=String::Trim::Regex::trim
   STR:t len10ws1: dataset=len10ws1 participant=String::Trim::Regex::trim
   TMX:m_a empty: dataset=empty participant=Text::Minify::XS::minify_ascii
   TMX:m_a len100ws1: dataset=len100ws1 participant=Text::Minify::XS::minify_ascii
   TMX:m_a len10ws1: dataset=len10ws1 participant=Text::Minify::XS::minify_ascii

To promote reusability, you can write scenarios in a Perl module, in the
C<Bencher::Scenario::*> namespace. In fact, the above scenario (with more
complete participants and datasets) is already on CPAN:
L<Bencher::Scenario::StringFunctions::Trim>. The examples below assume you have
installed it (with e.g. C<cpanm -n>).

=head2 Doing other things with scenario

=head3 Listing participants

Let's see which participants (functions, in this case) are included in the
scenario:

 % bencher -m StringFunctions::Trim --list-participants
 String::Trim::More::trim
 String::Trim::NonRegex::trim
 String::Trim::Regex::trim
 String::Util::trim
 Text::Minify::XS::minify

=head3 Listing datasets

What datasets are defined?

 % bencher -m StringFunctions/Trim --list-datasets
 empty
 len10ws1
 len100ws1
 len100ws10
 len100ws100
 len1000ws1
 len1000ws10
 len1000ws100
 len1000ws1000

=head3 Dumping the scenario

To see the scenario in more detail, let's dump it:

 % bencher -m StringFunctions::Trim --dump-parsed-scenario
 ...

=head3 Showing codes and results

Let's see how the functions will trim the data C<len100ws10>:

 % bencher -m StringFunctions/Trim --show-items-codes --include-dataset-name len10ws1
 #0 (participant=String::Trim::More::trim):
 package main; sub { String::Trim::More::trim(" xxxxxxxxxx ") }

 #1 (participant=String::Trim::NonRegex::trim):
 package main; sub { String::Trim::NonRegex::trim(" xxxxxxxxxx ") }

 #2 (participant=String::Trim::Regex::trim):
 package main; sub { String::Trim::Regex::trim(" xxxxxxxxxx ") }

 #3 (participant=String::Util::trim):
 package main; sub { String::Util::trim(" xxxxxxxxxx ") }

 #4 (participant=Text::Minify::XS::minify):
 package main; sub { Text::Minify::XS::minify(" xxxxxxxxxx ") }

Let's first check whether those functions do the job correctly:

 % bencher -m StringFunctions/Trim --show-items-results --include-dataset-name len10ws1
 #0 (participant=String::Trim::More::trim):
 "xxxxxxxxxx"

 #1 (participant=String::Trim::NonRegex::trim):
 "xxxxxxxxxx"

 #2 (participant=String::Trim::Regex::trim):
 "xxxxxxxxxx"

 #3 (participant=String::Util::trim):
 "xxxxxxxxxx"

 #4 (participant=Text::Minify::XS::minify):
 "xxxxxxxxxx"

=head3 Selecting only certain participants and datasets

Let's benchmark trimming short strings:

 % bencher -m StringFunctions/Trim --include-dataset-name len10ws1
 # Run on: perl v5.34.0, CPU Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz (2 cores), OS GNU/Linux Ubuntu version 20.04, OS kernel: Linux version 5.4.0-91-generic
 # Elapsed time: 0.13s
 +------------------------------+-----------+-----------+-----------------------+-----------------------+---------+---------+
 | participant                  | rate (/s) | time (ns) | pct_faster_vs_slowest | pct_slower_vs_fastest |  errors | samples |
 +------------------------------+-----------+-----------+-----------------------+-----------------------+---------+---------+
 | String::Util::trim           |    802126 |  1246.69  |                 0.00% |               651.62% |   0     |      20 |
 | String::Trim::Regex::trim    |    813180 |  1229.7   |                 1.38% |               641.40% | 1.1e-11 |      20 |
 | String::Trim::NonRegex::trim |    971000 |  1030     |                21.07% |               520.80% | 3.5e-10 |      28 |
 | String::Trim::More::trim     |   1250860 |   799.448 |                55.94% |               381.98% |   0     |      20 |
 | Text::Minify::XS::minify     |   6030000 |   166     |               651.62% |                 0.00% | 1.1e-10 |      20 |
 +------------------------------+-----------+-----------+-----------------------+-----------------------+---------+---------+

Let's see how L<Text::Minify::XS>C<::minify> perform on the different
datasets:

 % bencher -m StringFunctions/Trim --include-participant-name Text::Minify::XS::minify
 # Run on: perl v5.34.0, CPU Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz (2 cores), OS GNU/Linux Ubuntu version 20.04, OS kernel: Linux version 5.4.0-91-generic
 # Elapsed time: 0.18s
 +---------------+-----------+-----------+-----------------------+-----------------------+---------+---------+
 | dataset       | rate (/s) | time (μs) | pct_faster_vs_slowest | pct_slower_vs_fastest |  errors | samples |
 +---------------+-----------+-----------+-----------------------+-----------------------+---------+---------+
 | len1000ws1000 |    194823 |   5.13285 |                 0.00% |              3974.38% |   0     |      20 |
 | len1000ws100  |    426100 |   2.347   |               118.72% |              1762.85% | 3.9e-11 |      27 |
 | len1000ws10   |    500000 |   2       |               154.42% |              1501.46% | 2.5e-09 |      20 |
 | len1000ws1    |    520000 |   1.9     |               164.59% |              1439.91% | 6.4e-09 |      22 |
 | len100ws100   |   1528000 |   0.6545  |               684.30% |               419.49% | 4.6e-11 |      20 |
 | len100ws10    |   2690000 |   0.372   |              1279.83% |               195.28% | 4.6e-11 |      27 |
 | len100ws1     |   2950000 |   0.339   |              1413.54% |               169.20% | 4.4e-11 |      20 |
 | len10ws1      |   6130000 |   0.163   |              3045.75% |                29.52% | 1.1e-10 |      20 |
 | empty         |   7940000 |   0.126   |              3974.38% |                 0.00% | 6.3e-11 |      30 |
 +---------------+-----------+-----------+-----------------------+-----------------------+---------+---------+

=head3 Showing result in raw form

The result data is actually a data structure. You can show it as JSON and save it somewhere:

 % bencher -m StringFunctions/Trim --include-participant-name Text::Minify::XS::minify --format json
 % bencher -m StringFunctions/Trim --include-participant-name Text::Minify::XS::minify --json         ;# same thing
 [
    200,
    "OK",
    [
      {
         "dataset" : "empty",
         "ds_tags" : "",
         "errors" : 1.99650629822291e-10,
         "notes" : "",
         "p_tags" : "",
         "participant" : "Text::Minify::XS::minify",
         "perl" : "perl",
         "rate" : 7666873.39865677,
         "samples" : 34,
         "seq" : 0,
         "time" : 1.30431265524118e-07
      },
      ...

=head3 Saving and redisplaying result

 % bencher -m StringFunctions/Trim --include-participant-name Text::Minify::XS::minify --json > /path/to/result1.json

Later when you want to display it again, you can use L<bencher-fmt>:

 % bencher-fmt < /path/to/result1.json

=head2 Available subcommands

List all scenario modules (Bencher::Scenario::*) installed locally on your
system:

 % bencher --list-scenario-modules
 % bencher -L

Run benchmark described by a scenario module:

 % bencher -m Example

Run benchmark described by a scenario file:

 % bencher -f scenario.pl

Add participants from the command-line instead of (or in addition to) those
specified in a scenario file/module:

 % bencher -p '{"fcall_template":"Bar::func(<arg>)"}'

Run module startup overhead benchmark instead of the normal benchmark:

 % bencher -m Example --module-startup

Show/dump scenario instead of running benchmark:

 % bencher -m Example --show-scenario

List participants instead of running benchmark:

 % bencher ... --list-participants
 % bencher ... --list-participants -l ;# show detail

List participating Perl modules (modules mentioned by all the participants):

 % bencher ... --list-participant-modules
 % bencher ... --list-participant-modules -l ;# show detail

List datasets instead of running benchmark:

 % bencher ... --list-datasets
 % bencher ... --list-datasets -l ;# show detail

List items instead of running benchmark:

 % bencher ... --list-items
 % bencher ... --list-items -l ;# show detail

Show items' codes instead of running benchmark:

 % bencher ... --show-items-codes

Show items' results instead of running benchmark:

 % bencher ... --show-items-results

Select (include/exclude) participants before running benchmark (you can also
select datasets/modules/items):

 % bencher ... --include-participant-pattern 'Tiny|Lite' --exclude-participant 'HTTP::Tiny'

=head2 Benchmarking against multiple perls

You need to install L<App::perlbrew> first and then install some perls. Also,
install at least L<Bencher::Backend> to each perl you want to run the benchmark
on.

To list available perls (same as C<perlbrew list>, but also shows whether a perl
has Bencher):

 % bencher --list-perls
 % bencher --list-perls -l

To run a scenario against all perls which have Bencher:

 % bencher -m ScenarioModule --multiperl ...

To run a scenario against some perls:

 % bencher -m ScenarioModule --multiperl --include-perl perl-5.20.3 --include-perl perl-5.22.1 ...

=head2 Benchmarking multiple versions of a module

For example, if version 0.02 of a module is installed and you want to benchmark
against version 0.01 (in C</my/home/lib>):

 % bencher -m ScenarioModule --multimodver Module::Name -I /my/home/lib ...

Note that C<Module::Name> must be among the modules that are being benchmarked
(according to the scenario).


=head1 append:ENVIRONMENT

=head2 BENCHER_RESULT_DIR => str

Set default for C<--results-dir>.


=head1 SEE ALSO

L<bencher-tiny> if you want a simpler CLI with no non-core dependencies.

L<Bencher>

L<Bencher::Backend>

C<Bencher::Manual::*>
